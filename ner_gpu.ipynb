{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ercsonusharma/statistical-ml/blob/master/ner_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ur9o5xf3nOAc",
        "colab_type": "code",
        "outputId": "e77eef0c-24b6-41a8-acb2-a6bc361aea2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: <module 'tensorflow.test' from '/usr/local/lib/python2.7/dist-packages/tensorflow/test/__init__.py'>\n",
            "<module 'tensorflow.test' from '/usr/local/lib/python2.7/dist-packages/tensorflow/test/__init__.py'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7k_gEE7fMCNs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -r /content/results /content/drive/*/Colab-Note*/data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m_DCuLjKMNjq",
        "colab_type": "code",
        "outputId": "ce2e78d8-7bfd-4823-f93a-26693a537858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f2kivIkJMSVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf /content/results3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yVUmlBdjMtsa",
        "colab_type": "code",
        "outputId": "3cdcc688-0586-422f-eaba-1868b9c8f9b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /content/nutrition3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.npz\t\t   tags.txt\t    testb.words.txt  vocab.tags.txt\n",
            "negativeData.csv\t   testa.tags.txt   train.tags.txt   vocab.words.txt\n",
            "negativeNutritionData.txt  testa.words.txt  train.words.txt  words.txt\n",
            "SampleNutritionData.csv    testb.tags.txt   vocab.chars.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jjs_TkyLMw9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/My\\ Drive/Colab-Notebooks/nutrition3 /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6ksxJQFg_0O",
        "colab_type": "code",
        "outputId": "d6249112-755c-4ae9-fea6-4e457aca1548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /content/results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "main.log  model  params.json  score\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q5Zkm7erg_7y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ax51XZGwcWwi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf /content/results/model/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtxRSv_ytZS6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k0gV5Y04tZbI",
        "colab_type": "code",
        "outputId": "cb27833c-fde1-408b-b8f2-b1876e48ea83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3233
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import functools\n",
        "import json\n",
        "import logging\n",
        "from pathlib2 import Path\n",
        "import sys\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import metrics\n",
        "\n",
        "DATADIR = '/content/nutrition3/'\n",
        "\n",
        "# Logging\n",
        "Path('results3').mkdir(exist_ok=True)\n",
        "tf.logging.set_verbosity(logging.INFO)\n",
        "handlers = [\n",
        "    logging.FileHandler('results3/main.log'),\n",
        "    logging.StreamHandler(sys.stdout)\n",
        "]\n",
        "logging.getLogger('tensorflow').handlers = handlers\n",
        "\n",
        "def parse_fn(line_words, line_tags):\n",
        "    # Encode in Bytes for TF\n",
        "    words = [w.encode() for w in line_words.strip().split()]\n",
        "    tags = [t.encode() for t in line_tags.strip().split()]\n",
        "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
        "\n",
        "    # Chars\n",
        "    chars = [[c.encode() for c in w] for w in line_words.strip().split()]\n",
        "    lengths = [len(c) for c in chars]\n",
        "    max_len = max(lengths)\n",
        "    chars = [c + [b'<pad>'] * (max_len - l) for c, l in zip(chars, lengths)]\n",
        "    return ((words, len(words)), (chars, lengths)), tags\n",
        "\n",
        "\n",
        "def generator_fn(words, tags):\n",
        "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
        "        for line_words, line_tags in zip(f_words, f_tags):\n",
        "            yield parse_fn(line_words, line_tags)\n",
        "\n",
        "\n",
        "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
        "    params = params if params is not None else {}\n",
        "    shapes = ((([None], ()),               # (words, nwords)\n",
        "               ([None, None], [None])),    # (chars, nchars)\n",
        "              [None])                      # tags\n",
        "    types = (((tf.string, tf.int32),\n",
        "              (tf.string, tf.int32)),\n",
        "             tf.string)\n",
        "    defaults = ((('<pad>', 0),\n",
        "                 ('<pad>', 0)),\n",
        "                'O')\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        functools.partial(generator_fn, words, tags),\n",
        "        output_shapes=shapes, output_types=types)\n",
        "\n",
        "    if shuffle_and_repeat:\n",
        "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
        "\n",
        "    dataset = (dataset\n",
        "               .padded_batch(params.get('batch_size', 2500), shapes, defaults)\n",
        "               .prefetch(1))\n",
        "    return dataset\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Read vocabs and inputs\n",
        "    dropout = params['dropout']\n",
        "    (words, nwords), (chars, nchars) = features\n",
        "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    vocab_words = tf.contrib.lookup.index_table_from_file(\n",
        "        params['words'], num_oov_buckets=params['num_oov_buckets'])\n",
        "    vocab_chars = tf.contrib.lookup.index_table_from_file(\n",
        "        params['chars'], num_oov_buckets=params['num_oov_buckets'])\n",
        "    with Path(params['tags']).open() as f:\n",
        "        indices = [idx for idx, tag in enumerate(f) if tag.strip() != 'O']\n",
        "        num_tags = len(indices) + 1\n",
        "    with Path(params['chars']).open() as f:\n",
        "        num_chars = sum(1 for _ in f) + params['num_oov_buckets']\n",
        "\n",
        "    # Char Embeddings\n",
        "    char_ids = vocab_chars.lookup(chars)\n",
        "    variable = tf.get_variable(\n",
        "        'chars', [num_chars, params['dim_chars']], tf.float32)\n",
        "    char_embeddings = tf.nn.embedding_lookup(variable, char_ids)\n",
        "    char_embeddings = tf.layers.dropout(char_embeddings, rate=dropout,\n",
        "                                        training=training)\n",
        "\n",
        "    # Char LSTM\n",
        "    dim_words = tf.shape(char_embeddings)[1]\n",
        "    dim_chars = tf.shape(char_embeddings)[2]\n",
        "    flat = tf.reshape(char_embeddings, [-1, dim_chars, params['dim_chars']])\n",
        "    t = tf.transpose(flat, perm=[1, 0, 2])\n",
        "    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['char_lstm_size'])\n",
        "    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['char_lstm_size'])\n",
        "    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n",
        "    _, (_, output_fw) = lstm_cell_fw(t, dtype=tf.float32,\n",
        "                                     sequence_length=tf.reshape(nchars, [-1]))\n",
        "    _, (_, output_bw) = lstm_cell_bw(t, dtype=tf.float32,\n",
        "                                     sequence_length=tf.reshape(nchars, [-1]))\n",
        "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
        "    char_embeddings = tf.reshape(output, [-1, dim_words, 50])\n",
        "\n",
        "    # Word Embeddings\n",
        "    word_ids = vocab_words.lookup(words)\n",
        "    glove = np.load(params['glove'])['embeddings']  # np.array\n",
        "    variable = np.vstack([glove, [[0.] * params['dim']]])\n",
        "    variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n",
        "    word_embeddings = tf.nn.embedding_lookup(variable, word_ids)\n",
        "\n",
        "    # Concatenate Word and Char Embeddings\n",
        "    embeddings = tf.concat([word_embeddings, char_embeddings], axis=-1)\n",
        "    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n",
        "\n",
        "    # LSTM\n",
        "    t = tf.transpose(embeddings, perm=[1, 0, 2])  # Need time-major\n",
        "    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
        "    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
        "    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n",
        "    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n",
        "    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n",
        "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
        "    output = tf.transpose(output, perm=[1, 0, 2])\n",
        "    output = tf.layers.dropout(output, rate=dropout, training=training)\n",
        "\n",
        "    # CRF\n",
        "    logits = tf.layers.dense(output, num_tags)\n",
        "    crf_params = tf.get_variable(\"crf\", [num_tags, num_tags], dtype=tf.float32)\n",
        "    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Predictions\n",
        "        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n",
        "            params['tags'])\n",
        "        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n",
        "        predictions = {\n",
        "            'pred_ids': pred_ids,\n",
        "            'tags': pred_strings\n",
        "        }\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "    else:\n",
        "        # Loss\n",
        "        vocab_tags = tf.contrib.lookup.index_table_from_file(params['tags'])\n",
        "        tags = vocab_tags.lookup(labels)\n",
        "        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
        "            logits, tags, nwords, crf_params)\n",
        "        loss = tf.reduce_mean(-log_likelihood)\n",
        "\n",
        "        # Metrics\n",
        "        weights = tf.sequence_mask(nwords)\n",
        "        metrics = {\n",
        "            'acc': tf.metrics.accuracy(tags, pred_ids, weights)\n",
        "        }\n",
        "        for metric_name, op in metrics.items():\n",
        "            tf.summary.scalar(metric_name, op[1])\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.EVAL:\n",
        "            return tf.estimator.EstimatorSpec(\n",
        "                mode, loss=loss, eval_metric_ops=metrics)\n",
        "\n",
        "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            train_op = tf.train.AdamOptimizer().minimize(\n",
        "                loss, global_step=tf.train.get_or_create_global_step())\n",
        "            return tf.estimator.EstimatorSpec(\n",
        "                mode, loss=loss, train_op=train_op)\n",
        "if __name__ == '__main__':\n",
        "    # Params\n",
        "    params = {\n",
        "        'dim': 300,\n",
        "        'dim_chars': 100,\n",
        "        'dropout': 0.5,\n",
        "        'num_oov_buckets': 1,\n",
        "        'epochs': 5,\n",
        "        'batch_size': 2500,\n",
        "        'buffer': 15000,\n",
        "        'char_lstm_size': 25,\n",
        "        'lstm_size': 100,\n",
        "        'words': str(Path(DATADIR, 'vocab.words.txt')),\n",
        "        'chars': str(Path(DATADIR, 'vocab.chars.txt')),\n",
        "        'tags': str(Path(DATADIR, 'vocab.tags.txt')),\n",
        "        'glove': str(Path(DATADIR, 'glove.npz'))\n",
        "    }\n",
        "    with io.open(\"results3/params.json\",'w',encoding=\"utf-8\") as outfile:\n",
        "        outfile.write(unicode(json.dumps(params, ensure_ascii=False)))\n",
        "\n",
        "    def fwords(name):\n",
        "        return str(Path(DATADIR, '{}.words.txt'.format(name)))\n",
        "\n",
        "    def ftags(name):\n",
        "        return str(Path(DATADIR, '{}.tags.txt'.format(name)))\n",
        "\n",
        "    # Estimator, train and evaluate\n",
        "    train_inpf = functools.partial(input_fn, fwords('train'), ftags('train'),\n",
        "                                   params, shuffle_and_repeat=True)\n",
        "    eval_inpf = functools.partial(input_fn, fwords('testa'), ftags('testa'))\n",
        "\n",
        "    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n",
        "    ws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=DEFAULT_DIR+'results3/model')\n",
        "    estimator = tf.estimator.Estimator(model_fn, 'results3/model', cfg, params, warm_start_from=ws)\n",
        "    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n",
        "    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n",
        "        estimator, 'acc', 500, min_steps=2000, run_every_secs=120)\n",
        "    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n",
        "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "\n",
        "    # Write predictions to file\n",
        "    def write_predictions(name):\n",
        "        Path('results3/score').mkdir(parents=True, exist_ok=True)\n",
        "        with Path('results3/score/{}.preds.txt'.format(name)).open('wb') as f:\n",
        "            test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n",
        "            golds_gen = generator_fn(fwords(name), ftags(name))\n",
        "            preds_gen = estimator.predict(test_inpf)\n",
        "            for golds, preds in zip(golds_gen, preds_gen):\n",
        "                ((words, _), (_, _)), tags = golds\n",
        "                for word, tag, tag_pred in zip(words, tags, preds['tags']):\n",
        "                    f.write(b' '.join([word, tag, tag_pred]) + b'\\n')\n",
        "                f.write(b'\\n')\n",
        "\n",
        "    for name in ['train', 'testa', 'testb']:\n",
        "        write_predictions(name)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config: {'_save_checkpoints_secs': 120, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbf53f1c310>, '_model_dir': 'results3/model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
            "Not using Distribute Coordinator.\n",
            "Running training and evaluation locally (non-distributed).\n",
            "Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 120.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Create CheckpointSaverHook.\n",
            "Graph was finalized.\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Saving checkpoints for 0 into results3/model/model.ckpt.\n",
            "loss = 7.854328, step = 0\n",
            "global_step/sec: 1.1576\n",
            "loss = 0.24042156, step = 100 (86.392 sec)\n",
            "Saving checkpoints for 132 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:43:32\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-132\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:43:49\n",
            "Saving dict for global step 132: acc = 0.9993674, global_step = 132, loss = 0.022594795\n",
            "Saving 'checkpoint_path' summary for global step 132: results3/model/model.ckpt-132\n",
            "global_step/sec: 0.949289\n",
            "loss = 0.05322763, step = 200 (105.343 sec)\n",
            "Saving checkpoints for 248 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:45:33\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-248\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:45:50\n",
            "Saving dict for global step 248: acc = 0.999903, global_step = 248, loss = 0.0041397763\n",
            "Saving 'checkpoint_path' summary for global step 248: results3/model/model.ckpt-248\n",
            "global_step/sec: 0.935831\n",
            "loss = 0.020320896, step = 300 (106.857 sec)\n",
            "Saving checkpoints for 366 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:47:33\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-366\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:47:50\n",
            "Saving dict for global step 366: acc = 0.99995846, global_step = 366, loss = 0.0016646606\n",
            "Saving 'checkpoint_path' summary for global step 366: results3/model/model.ckpt-366\n",
            "global_step/sec: 0.951634\n",
            "loss = 0.014758185, step = 400 (105.086 sec)\n",
            "Saving checkpoints for 482 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:49:34\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-482\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:49:51\n",
            "Saving dict for global step 482: acc = 0.99997693, global_step = 482, loss = 0.00091846846\n",
            "Saving 'checkpoint_path' summary for global step 482: results3/model/model.ckpt-482\n",
            "global_step/sec: 0.937238\n",
            "loss = 0.011217004, step = 500 (106.695 sec)\n",
            "Saving checkpoints for 601 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:51:35\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-601\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:51:52\n",
            "Saving dict for global step 601: acc = 0.9999815, global_step = 601, loss = 0.0005796717\n",
            "Saving 'checkpoint_path' summary for global step 601: results3/model/model.ckpt-601\n",
            "global_step/sec: 0.950535\n",
            "loss = 0.007625604, step = 600 (105.195 sec)\n",
            "global_step/sec: 1.15981\n",
            "loss = 0.004907416, step = 700 (86.230 sec)\n",
            "Saving checkpoints for 718 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:53:35\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-718\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:53:53\n",
            "Saving dict for global step 718: acc = 0.9999815, global_step = 718, loss = 0.00040501333\n",
            "Saving 'checkpoint_path' summary for global step 718: results3/model/model.ckpt-718\n",
            "global_step/sec: 0.9578\n",
            "loss = 0.0058375634, step = 800 (104.406 sec)\n",
            "Saving checkpoints for 837 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:55:36\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-837\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:55:53\n",
            "Saving dict for global step 837: acc = 0.9999815, global_step = 837, loss = 0.00030340088\n",
            "Saving 'checkpoint_path' summary for global step 837: results3/model/model.ckpt-837\n",
            "global_step/sec: 0.946858\n",
            "loss = 0.0036284104, step = 900 (105.607 sec)\n",
            "Saving checkpoints for 955 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:57:36\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-955\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:57:53\n",
            "Saving dict for global step 955: acc = 0.9999862, global_step = 955, loss = 0.00022369702\n",
            "Saving 'checkpoint_path' summary for global step 955: results3/model/model.ckpt-955\n",
            "global_step/sec: 0.965851\n",
            "loss = 0.0023380814, step = 1000 (103.540 sec)\n",
            "Saving checkpoints for 1075 into results3/model/model.ckpt.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-17:59:37\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-1075\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-17:59:54\n",
            "Saving dict for global step 1075: acc = 0.99999076, global_step = 1075, loss = 0.00015859546\n",
            "Saving 'checkpoint_path' summary for global step 1075: results3/model/model.ckpt-1075\n",
            "Saving checkpoints for 1100 into results3/model/model.ckpt.\n",
            "Skip the current checkpoint eval due to throttle secs (120 secs).\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2018-11-27-18:00:14\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-1100\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Evaluation [10/100]\n",
            "Evaluation [20/100]\n",
            "Finished evaluation at 2018-11-27-18:00:30\n",
            "Saving dict for global step 1100: acc = 0.9999862, global_step = 1100, loss = 0.00015732941\n",
            "Saving 'checkpoint_path' summary for global step 1100: results3/model/model.ckpt-1100\n",
            "Loss for final step: 0.0023640052.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-1100\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-1100\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from results3/model/model.ckpt-1100\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DEwa9lRNtlzy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -r /content/results3 /content/drive/*/Colab-Note*/data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ADHO2dw3Mjc9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_fn_loc(line_words, line_tags):\n",
        "   # Encode in Bytes for TF\n",
        "    words = [w.encode() for w in line_words.strip().split()]\n",
        "    tags = [t.encode() for t in line_tags.strip().split()]\n",
        "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
        "\n",
        "    # Chars\n",
        "    chars = [[c.encode() for c in w] for w in line_words.strip().split()]\n",
        "    lengths = [len(c) for c in chars]\n",
        "    max_len = max(lengths)\n",
        "    chars = [c + [b'<pad>'] * (max_len - l) for c, l in zip(chars, lengths)]\n",
        "    yield ((words, len(words)), (chars, lengths)), tags    \n",
        "\n",
        "def input_fn_loc(words, tags, params=None):\n",
        "    params = params if params is not None else {}\n",
        "    shapes = ((([None], ()),               # (words, nwords)\n",
        "               ([None, None], [None])),    # (chars, nchars)\n",
        "              [None])                      # tags\n",
        "    types = (((tf.string, tf.int32),\n",
        "              (tf.string, tf.int32)),\n",
        "             tf.string)\n",
        "    defaults = ((('<pad>', 0),\n",
        "                 ('<pad>', 0)),\n",
        "                'O')\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        functools.partial(parse_fn_loc, words, tags),\n",
        "        output_shapes=shapes, output_types=types)\n",
        "\n",
        "    dataset = (dataset\n",
        "               .padded_batch(params.get('batch_size', 2500), shapes, defaults)\n",
        "               .prefetch(1))\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tw6XdsfRM2tx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "test_inpf = functools.partial(input_fn_loc, \"xzy free of milk\", \"O O O O\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0biHiQOJM2vy",
        "colab_type": "code",
        "outputId": "52338dec-f840-46a6-d4a4-b2e69ffcdca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "ws=tf.estimator.WarmStartSettings(ckpt_to_initialize_from='/content/results3/model')\n",
        "estimator = tf.estimator.Estimator(model_fn, '/content/results3/model',None, params,warm_start_from=ws)\n",
        "preds_gen = estimator.predict(test_inpf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using default config.\n",
            "Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbf53d30150>, '_model_dir': '/content/results3/model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p42hPDLIM5PP",
        "colab_type": "code",
        "outputId": "2c35dbf4-35c7-4ec0-fba0-be64e2e94e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "next(preds_gen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling model_fn.\n",
            "Done calling model_fn.\n",
            "Graph was finalized.\n",
            "Restoring parameters from /content/results3/model/model.ckpt-1100\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pred_ids': array([1, 2, 2, 3], dtype=int32),\n",
              " 'tags': array(['BQ-S', 'NV-D', 'NV-D', 'NV-M'], dtype=object)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "metadata": {
        "id": "EfjEY7vhM5RE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}